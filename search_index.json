[
["index.html", "EigenGWAS theory and application Chapter 1 EigenGWAS basis 1.1 Genetic relatedness matrix \\(\\mathbf{G}\\) 1.2 EigenGWAS linear model 1.3 Connection to singular value decomposition 1.4 Intepretation", " EigenGWAS theory and application Guo-Bo Chen [chen.guobo@foxmail.com] 2020-10-09 Chapter 1 EigenGWAS basis This project is dedicated to EigenGWAS, a linear model analysis approach for genomic data. EigenGWAS can be seamlessly integrated to the steps population genetic routines such as STRUCTRURE and principal component analysis. In a nutshell, it regresses the eigenvector against each marker, here often a single nucleotide polymorphism (SNP) marker, an analysis that detects the loci under selection. Its analysis pipeline may be found at RPub. 1.1 Genetic relatedness matrix \\(\\mathbf{G}\\) The genomic data can be represented as \\(\\mathbf{X}\\), an \\(n \\times m\\) matrix representing \\(n\\) individuals and \\(m\\) markers. Without loss of generality, \\(x_{il}\\) is the genotype code for the \\(i^{th}\\) individual at the \\(l^{th}\\) biallic locus. The data matrix \\(\\mathbf{X}\\) can be generated from chips, NGS, or GBS. In general, given \\(\\mathbf{X}\\) we can construct the \\(n\\times n\\) genetic relatedness matrix as \\[\\begin{equation} \\mathbf{G}=\\frac{1}{m}\\tilde{\\mathbf{X}}\\tilde{\\mathbf{X}}^T \\tag{1.1} \\end{equation}\\] in which \\(\\tilde{\\mathbf{X}}\\) is the scaled form of \\(\\mathbf{X}\\) (slighlty different from standardization, which will be \\(\\frac{x_l-2p_l}{\\sqrt{2(1+F_l)p_lq_l}}\\)). However, upon the mating type of the species, \\(\\mathbf{G}\\) should be constructed accordingly. For a random mating population, \\(x_l\\) is scaled as \\(\\tilde{x}_l=\\frac{x_l-2p_l}{\\sqrt{2p_lq_l}}\\), assuming the current population as the base population, whereas for an inbred population, \\(\\tilde{x}_l=\\frac{x_l-2p_l}{\\sqrt{4p_lq_l}}\\), and \\(q_l=1-p_l\\) the frequency for the alternative allele. Due to missing data, in practice for a pair of individuals \\(i\\) and \\(j\\), their pairwise relatedness is \\[\\begin{equation} G_{ij}=\\frac{1}{\\tilde{m}}\\sum_l^{\\tilde{m}}\\frac{(x_{il}-2p_l)(x_{jl}-2p_l)}{2(1+F)p_lq_l} \\tag{1.2} \\end{equation}\\] in which \\(\\tilde{m}\\) is the number of loci genotyped at both individal \\(i\\) and \\(j\\), and \\(F\\) the inbreeding coefficient taking the value of 0 for random mating population and 1 for inbred population. When \\(F\\) is set as zero, it is assumed that the current population is the base population. 1.1.1 Statistical properties of \\(\\mathbf{G}\\) Let \\(\\mathbf{G}_o\\) denote the off-diagonal elements of \\(\\mathbf{G}\\), then we can define two population parameters, \\(n_e\\), the effective population size, and \\(m_e\\), the effective number of markers. \\[\\begin{equation} n_e=\\frac{-1}{mean(\\mathbf{G}_o)} \\tag{1.3} \\end{equation}\\] \\(n_e\\) reflects true relatedness between any pair of samples; \\[\\begin{equation} m_e=\\frac{1}{Var(\\mathbf{G}_o)} \\tag{1.4} \\end{equation}\\] The ratio between \\(\\frac{m_e}{m}\\) reflects the average linkage disequilibrium between the any pair of markers, and alternatively \\(m_e\\) can be expressed as \\[\\begin{equation} m_e=\\frac{m^2}{\\sum_{l_1=1}^m\\sum_{l_2=1}^m\\rho_{l_1l_2}^2}=\\frac{1}{\\bar{\\rho}^2} \\tag{1.5} \\end{equation}\\] in which \\(\\rho_{l_1l_2}\\) is Pearson’s correlation between a pair of SNPs, see Appendix 7.1. It is an important parameter to characterize the evolutionary process of a population. 1.2 EigenGWAS linear model Given eigenanalysis (see wikipedia for its more details) of \\(\\mathbf{X}\\), we have \\(\\mathbf{E}\\) and \\(\\mathbf{\\Lambda}\\), in which \\(\\mathbf{\\Lambda}\\) is an \\(n \\times n\\) diagonal matrix for eigenvalues and \\(\\mathbf{E}\\) is an \\(n \\times n\\) matrix for the eigenvectors. \\(\\mathbf{E}_k\\) is the \\(k^{th}\\) eigenvector associated with the \\(k^{th}\\) largest eigenvalue. Regressing \\(\\mathbf{E}_k\\) against the \\(l^{th}\\) marker, we have the model below \\[\\begin{equation} \\mathbf{E}_k=a+\\beta_l\\mathbf{x}_l+e \\tag{1.6} \\end{equation}\\] It consequently generates \\(m\\) estimates of \\(\\hat{\\beta}\\), \\(\\hat{\\sigma}_{\\beta}\\), and their corresponding \\(p\\) values. The \\(p\\) value can be converted to one-degree-of-freedom \\(\\chi^2_1\\). Under the context of EigenGWAS, the \\(\\chi^2_1\\) can be intepreted as \\[\\begin{equation} 4\\frac{\\color{red}{n_1}\\color{blue}{n_2}}{n}\\frac{(\\color{red}{p_{1,l}}-\\color{blue}{p_{2,l}})^2}{2p_lq_l}=4n\\omega_1 \\omega_2 F_{st}^N=nF_{st}^W \\tag{1.7} \\end{equation}\\] in which \\(\\color{red}{n_1}\\) and \\(\\color{blue}{n_2}\\) are the numbers of samples at the left and right side of “0” on the eigenvector (see the figure below), and \\(\\omega_g=\\frac{n_g}{n}\\) the proportion of a subgroup in the sample (\\(g=2\\) in EigenGWAS analysis), and \\(\\color{red}{p_{1,l}}\\) and \\(\\color{blue}{p_{2,l}}\\) are the frequencies of the reference allele in two subgroups, respectively. \\(F_{st}^N=\\frac{(\\color{red}{p_{1,l}}-\\color{blue}{p_{2,l}})^2}{2p_lq_l}\\) and \\(F_{st}^W=2\\frac{\\sum_{g=1}^2\\omega_g(p_{g,l}-p_l)^2}{p_lq_l}\\). It is obviously that when \\(n_1=n_2=\\frac{n}{2}\\) the test statistic reaches its maximum. ## [1] 100 1000 ## [1] &quot;Ne= 100 Me= 1080.91076380488 given N= 100 and M= 1000&quot; 1.2.1 \\(\\lambda_{GC}\\) correction Given the median, denoted as \\(\\chi^2_{1,median(p)}\\), of the \\(m\\) \\(\\chi^2_1\\) values observed, we can define \\(\\lambda_{GC}=\\chi^2_{1,median(p)}/\\chi^2_{1,0.5}\\), in which \\(\\chi^2_{1,0.5}=0.455\\). We further use subscript \\(k\\) to denote \\(\\lambda_{GC_k}\\) the one that is estimated from the EigenGWAS analysis of \\(\\mathbf{E}_k\\), as shown (1.6). After technical correction, correspondingly \\[\\begin{equation} \\tilde\\chi^2_1=\\chi^2_1/\\lambda_{GC_k} \\tag{1.8} \\end{equation}\\] a correction of the test statistic. Compared with its original form, the correction has several implications Statistically, as (1.6) has its response variable from \\(\\mathbf{X}\\), the correction removes its overfitting. Genetically, it corrects for genetic drift such as soaked in \\(\\mathbf{E}_1\\). Here the quantity of the genetic drift is measured by the median of the \\(m\\) \\(\\chi^2_1\\) values observed. \\(\\lambda_{GC}\\) parameter has long been proposed in controlling population structure in GWAS, but finds its best fit in EigenGWAS. In practice, the correction can be further refined by quantile-based \\(\\lambda_{GC}\\) rather than the single median p-value only. 1.2.2 Threshold for EigenGWAS As shown above, EigenGWAS is a linear model framework nearly identical to the conventional GWAS, and Bonferroni correction, such as \\(\\alpha/m\\), can be used to set the threshold at the significance level \\(\\alpha\\), such as \\(\\alpha=0.05\\). The loci that exceeds the threshold are under selection. 1.3 Connection to singular value decomposition Singular value decomposition (SVD, see wiki for more details) can decompose the matrix \\(\\mathbf{X}\\) into \\[\\begin{equation} \\mathbf{X}=\\mathbf{U\\Sigma V} \\tag{1.9} \\end{equation}\\] in which \\(\\mathbf{U}\\) is an \\(n\\times n\\) unitary matrix, corresponding to individual-level loading for each sample. \\(\\mathbf{U}\\) is also called left-singular vectors. \\(\\mathbf{\\Sigma}\\) an \\(n\\times m\\) diagonal matrix for singular values which are square roots of the eigenvalue (\\(\\lambda_k\\)) of GRM matrix. \\(\\mathbf{V}\\) an \\(m\\times m\\) unitary matrix, corresponding to SNP-level loading. \\(\\mathbf{U}\\) is also called right-singular vectors. Due to the transformation between \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\), it has \\[\\begin{equation} \\mathbf{U}=\\mathbf{\\tilde{X}V\\Sigma}^{-1} \\tag{1.10} \\end{equation}\\] The right side of (1.10) can be unfolded as \\(\\frac{\\sqrt{m}}{\\Sigma_k}\\mathbf{\\tilde{x}}_l\\mathbf{v}_k\\), following \\(N(0,1)\\), and taking square of it leads to \\[\\begin{equation} (\\frac{\\sqrt{m}}{\\Sigma_k}\\mathbf{\\tilde{x}}_l\\mathbf{v}_k)^2 \\sim \\tilde{\\chi}^2_{1,svd} \\tag{1.11} \\end{equation}\\] It brings out an interesting comparison between (1.8) and (1.10), and exists subtle difference, for instance \\(E_1\\), in their correction using \\(\\lambda_{GC}\\) and \\(\\lambda_1\\). When the population divergency is due to genetic drift and a small proportion of loci are under selection, \\(\\lambda_1 \\gt \\lambda_{GC}\\). It is because \\(\\lambda_1\\) is the mean of the \\(\\chi^2_1\\) from (1.7), but \\(\\lambda_{GC}\\) is the median of it. Given a population under selection, such as selection sweep, it does have \\(\\lambda_1 \\gt \\lambda_{GC}\\), as demonstrated in simulation. So, using linear model system such as EigenGWAS brings more flexibility, as well as improved statistical power comparing with direct left-to-right singular vector transformation in SVD (1.10). However, in this transformation, eigenvalue is involved, as would be show. It will reduce the statistical power for EigenGWAS. In theory and practice, with type-I error rate well-controlled, \\(\\tilde{\\chi}^2_1\\) (1.8) is greater than \\(\\tilde{\\chi}^2_{1.svd}\\) (1.10). 1.4 Intepretation 1.4.1 \\(F_{st}\\) EigenGWAS provides an unsupervised realization for \\(F_{st}\\) the conventional implementation of that assumes the underlying subgroups are known. In practice, however, it is often unclear how the possible subgroups should be defined. In contrast, EigenGWAS implicitly partitions the samples into two subgroups at each eigenvector at zero point on the coordinate. No guarantee that this strategy optimal, it seems a sound attempt when the group information is missing. 1.4.2 Classic mechanic intepretation The early framework of population genetics indebts a lot for theoretical physics (Ronald Fisher studied astronomy). As a linear model system, EigenGWAS also finds its analogue in Newtown’s first and the second laws for classical mechanics. The first law states “In an inertial frame of reference, an object either remains at rest or continues to move at a constant velocity, unless acted upon by a force.” In population genetics, it can be seemed as genetic drift that is constantly driving the a pair of population apart from each other, and its velocity can be quantified by a binomial distribution as \\(\\frac{pq}{\\tilde{n}_e}\\). The second law stats “In an inertial frame of reference, the vector sum of the forces \\(F\\) on an object is equal to the mass m of that object multiplied by the acceleration a of the object: \\(F = ma\\).” The selection can drive a genomic region run against its reference population at a velocity greater than \\(\\frac{pq}{\\tilde{n}_e}\\). Although, this interpretation works even for domesticated populations, its proportion of the markers divergent by genetic drift and selection would be very different. "],
["protocols.html", "Chapter 2 Protocols 2.1 Protocols for selection 2.2 Protocal for EigenGWAS 2.3 Protocol for predicted eigenvectors 2.4 Nucleotide diversity 2.5 Other test statistics for selection", " Chapter 2 Protocols 2.1 Protocols for selection Constructing genetic relationship matrix \\(\\mathbf{G}\\). The difference between a random mating population and inbred population is the way \\(\\mathbf{G}\\) is constructed. So for a pair of individuals \\(i\\) and \\(j\\), \\[G_{ij}=\\frac{1}{\\tilde{m}}\\sum_l^{\\tilde{m}}\\frac{(x_{il}-2p_l)(x_{jl}-2p_l)}{2(1+F)p_lq_l}\\] and \\(F=1\\) for inbred lines. Please refer to Chapter 1. Conducting eigenanalysis for \\(\\mathbf{G}\\), see it definition (1.2). Linear regression analysis for each SNP. 2.2 Protocal for EigenGWAS A simple analysis pipeline written in Rscripts may be found at Rpub and its Shiny demo. Or, try it as below in RStudio runGitHub(&quot;Rshiny&quot;, &quot;gc5k&quot;) 2.3 Protocol for predicted eigenvectors The prediction accuracy can be written as \\[R^2 \\approx \\frac{1}{1+\\frac{n_e}{m}}\\] 2.4 Nucleotide diversity \\(\\theta\\), the normalized observed number of variants sites \\(\\pi\\), the observed heterozygosity per base pair Nonetheless, we can expect that favored alleles will generally sit whthin large shared haplotypes, and that these haplotypes will be in sharp contrast with more variable haplotypes on the unselected background. Voight, et al, PLoS Biology, 2006, 4, e72. However, the excess variability around the site of F/S amino-acid polymorphism is found within S haplotypes, and F haplotypes are depauperate in variability, suggesting that the F allele is a derived variant that has recently swept to an intermediate frequency. Charsworth &amp; Charsworth, Heredity, 2017, 118:2-9. 2.5 Other test statistics for selection 1 Tajima’s \\(D\\), Statistical method for testing the neutral mutation hypothesis by DNA polymorphism, 1989, \\(Genetics\\), 123:585-95 2 Fu &amp; Li’s \\(D\\), Statistical tests of neutrality of mutations, 1993, \\(Genetics\\), 133:693-709 3 Fay &amp; Wu’s \\(H\\), Hitchhiking under positive Darwinian selection, 2000, \\(Genetics\\), 155:1405-13 4 McDonald and Kreitman test, Adaptive protein evolution at the Adh locus in Drosophila, 1991, \\(Nature\\), 335:167-70 5 HKA test, Hudson, Kreitman, Aguade, A test of neutral molecular evolution based on nucleotide data, 1987, \\(Genetics\\), 166:153-9) 6 Extended haplotype homozygosity (EHH) The biological ground please refer to \\(Nature\\), 419:832-7 and the definition of the test statistic please refer to Voight et al \\(PLoS Biology, 2006, 4, e72\\) Unstandardized \\(iHS=ln\\frac{iHH_A}{iHH_D}\\) and standardized \\(iHS=\\frac{ln[\\frac{iHH_A}{iHH_D}]-E_p[ln(\\frac{iHH_A}{iHH_D})]}{SD_P[ln(\\frac{iHH_A}{iHH_D})]}\\) It has been realized in rehh: an R package to detect footprints of selection in genome-wide SNP data from haplotype structure, \\(Bioinformatics\\), 2012, 8:1176-7 selscan: An Efficient Multithreaded Program to Perform EHH-Based Scans for Positive Selection, 2014, \\(MBE\\), 31:2824-7, selcan’s github PopGenome, R package:, Pfeifer, B. et al. (2014) PopGenome: An Efficient Swiss Army Knife for Population Genomic Analyses in R. Mol Biol Evol 31(7): 1929-1936. "],
["resequencing-studies.html", "Chapter 3 Resequencing studies 3.1 Technical review 3.2 Drop HWE test", " Chapter 3 Resequencing studies This is a technical review for resequencing studies. 3.1 Technical review 3.1.1 Simulation Bifurcating genealogy using Hudson’s coalescent theory (for a mini review please see Richard Hudson and Norman Kaplan on the Coalescent Process, \\(Genetics\\), 2016, 202, 865-6) Kelleher, Etheridge, and Mcvean, Efficient Coalescent Simulation and Genealogical Analysis for Large Sample Sizes, \\(PLoS Computational Biology\\), 2016, 12:e1004842 3.1.2 NGS 3.1.3 Chip data 3.1.4 GBS 3.2 Drop HWE test "],
["simulating-population-structure.html", "Chapter 4 Simulating population structure 4.1 Genetic drift 4.2 Discrete populations 4.3 Admixture populations 4.4 Homo &amp; Heteogeneous \\(F_{st}\\) 4.5 Wishart distribution 4.6 Tracy-Widom distribution", " Chapter 4 Simulating population structure A toy example for this Charpter can be found in gc5k’s Rpub 4.1 Genetic drift As each locus follows binomial distribution, the genetic drift can be modelled \\(\\frac{\\sqrt{pq}}{2n_e}\\), in which \\(n_e\\) is the effective population size. 4.2 Discrete populations Algorithm Generate frequency \\(f\\) from the uniform distribution \\((0.05, 0.95)\\). Given \\(F_{st1}\\), generating \\(z_{1|1}\\) and \\(z_{2|1}\\) from \\(Beta(f\\frac{1-F_{st1}}{F_{st1}}, (1-f)\\frac{1-F_{st1}}{F_{st1}})\\), respectively. The mean of them will be \\(f\\), and their sampling variance will be \\(F_{st}\\). Similarly, generate \\(z_{1|2}\\) and \\(z_{2|2}\\). Set \\(D_1\\) and \\(D_2\\), the realized frequencies of the two populations. For the three-population simulation \\[\\begin{equation} F= \\left ( \\begin{array}{cc} 1 &amp; 0\\\\ 0 &amp; 1 \\\\ 0 &amp; 1\\\\ \\end{array} \\right ) \\left ( \\begin{array}{c} z_{1|1}\\\\ z_{2|1}\\\\ \\end{array} \\right ) + \\left ( \\begin{array}{cc} 0 &amp; 0\\\\ 1 &amp; 0 \\\\ 0 &amp; 1\\\\ \\end{array} \\right ) \\left ( \\begin{array}{c} z_{1|2}\\\\ z_{2|2}\\\\ \\end{array} \\right) \\end{equation}\\] Below are three simulations generated from 3, 5, and 9 populations, each of which has 100 individuals; 10000 markers are used for each individual. \\(D_1\\) and \\(D_2\\) are printed for the three simulations below. For the five-population simulation, \\[\\begin{equation} F=\\left ( \\begin{array}{cc} 1 &amp; 0 \\\\ 0.5 &amp; 0.5 \\\\ 0.5 &amp; 0.5 \\\\ 0.5 &amp; 0.5 \\\\ 0 &amp; 1\\\\ \\end{array} \\right ) \\left ( \\begin{array}{c} z_{1|1}\\\\ z_{2|1}\\\\ \\end{array} \\right ) + \\left (\\begin{array}{cc} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 0 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0\\\\ \\end{array} \\right) \\left( \\begin{array}{c} z_{1|2}\\\\ z_{2|2}\\\\ \\end{array} \\right) \\end{equation}\\] For the nine-population simulation, scheme 1 \\[\\begin{equation} F=\\left ( \\begin{array}{cc} 1 &amp; 0 \\\\ 0.3 &amp; 0.7 \\\\ 0.3 &amp; 0.7 \\\\ 0.5 &amp; 0.5 \\\\ 0.5 &amp; 0.5 \\\\ 0.5 &amp; 0.5 \\\\ 0.7 &amp; 0.3 \\\\ 0.7 &amp; 0.3 \\\\ 0 &amp; 1 \\\\ \\end{array} \\right) \\left( \\begin{array}{c} z_{1|1}\\\\ z_{2|1}\\\\ \\end{array} \\right) + \\left ( \\begin{array}{cc} 0 &amp; 0 \\\\ 0.42 &amp; 0.18 \\\\ 0.18 &amp; 0.42 \\\\ 1 &amp; 0 \\\\ 0 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0.42 &amp; 0.18 \\\\ 0.18 &amp; 0.42 \\\\ 0 &amp; 0 \\\\ \\end{array} \\right ) \\left ( \\begin{array}{c} z_{1|2}\\\\ z_{2|2}\\\\ \\end{array} \\right ) \\end{equation}\\] For the nine-population simulation, scheme 2 \\[\\begin{equation} F=\\left ( \\begin{array}{cc} 1 &amp; 0 \\\\ 0.3 &amp; 0.7 \\\\ 0.3 &amp; 0.7 \\\\ 0.5 &amp; 0.5 \\\\ 0.5 &amp; 0.5 \\\\ 0.5 &amp; 0.5 \\\\ 0.7 &amp; 0.3 \\\\ 0.7 &amp; 0.3 \\\\ 0 &amp; 1 \\\\ \\end{array} \\right) \\left( \\begin{array}{c} z_{1|1}\\\\ z_{2|1}\\\\ \\end{array} \\right) + \\left ( \\begin{array}{cc} 0 &amp; 0 \\\\ 0.3 &amp; 0.0 \\\\ 0.0 &amp; 0.3 \\\\ 1 &amp; 0 \\\\ 0 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0.3 &amp; 0.0 \\\\ 0.0 &amp; 0.3 \\\\ 0 &amp; 0 \\\\ \\end{array} \\right ) \\left ( \\begin{array}{c} z_{1|2}\\\\ z_{2|2}\\\\ \\end{array} \\right ) \\end{equation}\\] 4.3 Admixture populations 4.4 Homo &amp; Heteogeneous \\(F_{st}\\) 4.5 Wishart distribution R function rWishart can generate Wishart distribution easiliy. ## Artificial S &lt;- toeplitz((10:1)/10) set.seed(11) R &lt;- rWishart(1000, 20, S) dim(R) # 10 10 1000 ## [1] 10 10 1000 4.6 Tracy-Widom distribution R package RMTstat can help study Tracy-Widom distribution. library(RMTstat) plot(density(rtw(1000)), main=&quot;TW distribbtion&quot;, bty=&#39;n&#39;) "],
["data-analysis.html", "Chapter 5 Data analysis 5.1 On-site examples 5.2 Public datahub (NEO) 5.3 Meta-scale", " Chapter 5 Data analysis 5.1 On-site examples 5.1.1 Arabdiopsis 1,135 genomes reveal the global pattern of polymorphism in Arabidopsis thaliana, 2016, \\(Cell\\), 166:481-91 Reproduction and In-Depth Evaluation of Genome-Wide Association Studies and Genome-Wide Meta-analyses Using Summary Statistics, 2017, \\(G3\\), 7:943-92 5.1.2 3K rice Genomic variation in 3,010 diverse accessions of Asian cultivated rice, 2018, \\(Nature\\), 557:43-9 5.1.3 ALS Genomic dissection of population substructure of Han Chinese and its implication in association studies, \\(AJHG\\), 2009, 85:762-74 Genomic analyses from non-invasive prenatal testing reveal genetic associations, patterns of viral infections, and Chinese population history, \\(Cell\\), 2018, 175:347-59 Darwinian Positive Selection on the Pleiotropic Effects of KITLG Explain Skin Pigmentation and Winter Temperature Adaptation in Eurasians, \\(MBE\\), 2018, 35:2272-2283 5.1.4 GF 5.1.5 UK birds Recent natural selection causes adaptive evolution of an avian polygenic trait, \\(Science\\), 2017, 358:365-8 5.1.6 Darwin’s finches Evolution of Darwin’s finches and their beaks revealed by genome sequencing, \\(Nature\\), 2015, 518, 371-5 5.1.7 GBS Maize 5.1.8 UK Biobank The UK Biobank resource with deep phenotyping and genomic data, \\(Nature\\), 2018, 562:203-9 5.2 Public datahub (NEO) 5.3 Meta-scale 5.3.1 \\(n_e\\) 5.3.2 \\(m_e\\) "],
["conclusion.html", "Chapter 6 Conclusion 6.1 Statistical power 6.2 Selection pattern", " Chapter 6 Conclusion 6.1 Statistical power The key parameters are below. The validation for NCP for inbred populations \\[n\\omega_1\\omega_2\\frac{(p_1-p_2)^2}{p(1-p)}\\] Simulation validation for NCP RP=1000 n1=500 n2=500 N=n1+n2 f1=0.4 f2=0.6 paraA=matrix(0, RP, 1) for(i in 1:RP) { g1=rbinom(n1, 1, f1)*2 g2=rbinom(n2, 1, f2)*2 y=c(rep(1,n1), rep(0, n2)) ys=scale(y) G=c(g1, g2) Ga=c(g1, g2) modA=lm(ys~Ga) paraA[i,1]=summary(modA)$coefficients[2,3]^2 } ncpA=N*n1/N*n2/N*(mean(g1)/2-mean(g2)/2)^2/(mean(Ga)/2*(1-mean(Ga)/2)) qqplot(main=&quot;&quot;, rchisq(RP,1, ncp = ncpA), paraA[,1], pch=16, cex=0.5, bty=&#39;n&#39;, xlab=expression(paste(&quot;Theoretical &quot;, chi[1]^2)), ylab=expression(paste(&quot;Observed &quot;,chi[1]^2))) abline(a=0, b=1, lty=2, col=&quot;grey&quot;) For random mating population, the NCP for the additive model is approximately \\[4n\\omega_1\\omega_2\\frac{(p_1-p_2)^2}{2p(1-p)}\\] in which \\(p=\\omega_1p_1+\\omega_2p_2\\), and \\(\\omega_1=\\frac{n_1}{n_1+n_2}\\) and \\(\\omega_2=\\frac{n_2}{n_1+n_2}\\); for the dominance model is \\[n\\omega_1\\omega_2\\frac{[2p_1(1-p_1)-2p_2(1-p_2)]^2}{2p_1(1-p_1)\\omega_1+2p_2(1-p_2)\\omega_2}\\] A shiny power calculator may be found here. RP = 500 n1 = 500 n2 = 500 N = n1 + n2 f1 = 0.4 f2 = 0.6 para = matrix(0, RP, 6) paraA = matrix(0, RP, 6) for (i in 1:RP) { g1 = rbinom(n1, 2, f1) g2 = rbinom(n2, 2, f2) y = c(rep(1, n1), rep(0, n2)) ys = scale(y) G = c(g1, g2) Gd = ifelse(G == 1, 1, 0) Ga = c(g1, g2) # Gd=scale(Gd) mod = lm(ys ~ Gd) Ecov = sqrt(n1/N * n2/N) * (length(which(g1 == 1))/n1 - length(which(g2 == 1))/n2) EV = mean(Gd) * (1 - mean(Gd)) Eb = Ecov/EV b = mod$coefficients[2] para[i, 1] = Eb para[i, 2] = b para[i, 3] = sqrt(1/(N * var(Gd))) para[i, 4] = summary(mod)$coefficients[2, 2] para[i, 5] = summary(mod)$coefficients[2, 3]^2 para[i, 6] = summary(mod)$coefficients[2, 4] modA = lm(ys ~ Ga) paraA[i, 5] = summary(modA)$coefficients[2, 3]^2 } # layout(matrix(1:2, 1, 2)) vF2=f1*(1-f1)*n1/N+f2*(1-f2)*n2/N ncpD=n1*n2/N * # (2*f1*(1-f1)-2*f2*(1-f2))^2/vF2 qqplot(main=&#39;Dom&#39;, rchisq(RP,1, ncp = # ncpD), para[,5], pch=16, cex=0.5, bty=&#39;n&#39;, # xlab=expression(paste(&#39;Theoretical &#39;, chi[1]^2)), # ylab=expression(paste(&#39;Obs &#39;,chi[1]^2))) abline(a=0, b=1) ncpA = 4 * N * n1/N * n2/N * (mean(g1)/2 - mean(g2)/2)^2/(2 * mean(Ga)/2 * (1 - mean(Ga)/2)) qqplot(main = &quot;&quot;, rchisq(RP, 1, ncp = ncpA), paraA[, 5], pch = 16, cex = 0.5, bty = &quot;n&quot;, xlab = expression(paste(&quot;Theoretical &quot;, chi[1]^2)), ylab = expression(paste(&quot;Observed &quot;, chi[1]^2))) abline(a = 0, b = 1, lty = 2, col = &quot;grey&quot;) m=1000000 alpha=0.05 pcut=alpha/m chiT=qchisq(pcut, 1, lower.tail = F) n=c(100, 200, 500, 1000, 1500, 2000, 5000, 7500, 10000, 15000, 20000, 50000) PW=matrix(0, 2, length(n)) w1=0.3 w2=1-w1 p1=0.35 h1=2*p1*(1-p1) p2=0.5 h2=2*p2*(1-p2) p=w1*p1+w2*p2 H=w1*h1+w2*h2 for(i in 1:length(n)) { ncpA=4*n[i]*w1*w2*(p1-p2)^2/(2*p*(1-p)) ncpD=n[i]*w1*w2*(h1-h2)^2/H PW[1,i]=pchisq(chiT, 1, ncp=ncpA, lower.tail = F) PW[2,i]=pchisq(chiT, 1, ncp=ncpD, lower.tail = F) } colnames(PW)=n barplot(PW, beside = T, border = F) abline(h=0.85, lty=2, col=&quot;grey&quot;) legend(&quot;topleft&quot;, legend=c(&quot;Add&quot;, &quot;Dom&quot;), pch=15, col=c(&quot;black&quot;, &quot;grey&quot;), bty=&#39;n&#39;) 6.2 Selection pattern "],
["app-1.html", "Chapter 7 Appendix 1 7.1 Notes on linkage disequilibrium 7.2 The maximal value \\(\\rho_{AB}\\) 7.3 LD and recombination fraction 7.4 \\(F_{st}\\)", " Chapter 7 Appendix 1 7.1 Notes on linkage disequilibrium For the haplotypes, as tabulated below, consisting of locus 1, which has alleles \\(A\\) and \\(a\\), and locus 2, which has alleles \\(B\\) and \\(b\\), \\(B\\) \\(b\\) \\(A\\) \\(P_{AB}=P_A \\times P_B + D\\) \\(P_{Ab}=P_A \\times P_b - D\\) \\(P_A\\) \\(a\\) \\(P_{aB}=P_a \\times P_B - D\\) \\(P_{ab}=P_a \\times P_b + D\\) \\(P_a\\) \\(P_B\\) \\(P_b\\) It is a typical \\(2\\times2\\) contingency table, once one of these four cells is fixed, other three are all known. Consequently, we can implement Fisher’s Exact test for this table. However, given the current biotechnology, \\(D\\) is often unknown. But given the correlation, \\(\\rho_{1,2}\\), of a pair of markers, we know that \\[\\rho_{1,2}=\\frac{D_{1,2}}{\\sqrt{P_AP_aP_BP_b}}\\] In simulation, it is very easy to simulate LD given the correlation. Calculate \\(D_{1,2}=\\rho_{1,2}\\sqrt{P_AP_aP_BP_b}\\) first. Depending on the allele at locus 1, the probabilities of observing allele \\(B\\) and \\(b\\) are \\(P(B|A)=\\frac{P_{AB}}{P_{A}} =\\frac{P_A\\times P_B+D}{P_A}\\), \\(P(b|A)=1-P(B|A)\\), \\(P(b|a)=\\frac{P_{ab}}{P_a} =\\frac{P_a\\times P_b+D}{P_a}\\), and \\(P(B|a)=1-P(B|a)\\), respectively. So, the recombination can be defined as \\(\\frac{P(B|A)}{P(B|A) + P(b|A)}=P_B + \\frac{D}{P_A}\\). 7.2 The maximal value \\(\\rho_{AB}\\) Assuming \\(P_A \\leq P_B \\leq 0.5\\), the maximal of \\(\\rho_{l_1l_2}\\) reaches when allele \\(A\\) is \\(a\\) subset of \\(B\\) linage so that \\(P_{AB}=P_{A}\\), because (see PLoS Genet 2(9): e142) \\[\\rho_{1,2}=\\frac{P_{AB}-P_{A}P_{B}}{\\sqrt{P_A P_a P_B P_b}}=\\frac{P_A(1-P_B)}{\\sqrt{P_A P_a P_B P_b}}=\\sqrt{\\frac{P_AP_b}{P_B P_a}} \\leq 1\\] The maxima of \\(D=min(P_AP_b, P_BP_a)\\). The conditional probability for a pair of biallelic loci \\(a_l\\) \\(A_l\\) \\(a_k\\) \\(r_{kl}=q_l + \\frac{D_{kl}}{q_k}\\) 1-\\(r_{kl}=p_l - \\frac{D_{kl}}{q_k}\\) \\(q_k\\) \\(A_k\\) 1-\\(R_{kl}=q_l - \\frac{D_{kl}}{p_k}\\) \\(R_{kl}=p_l +\\frac{D_{kl}}{p_k}\\) \\(p_k\\) \\(q_l\\) \\(p_l\\) Often, there is another commonly statistic, called \\(D^{\\prime}=\\frac{\\tilde{D}}{min(P_AP_b, P_BP_a)}\\), in which \\(\\tilde{D}\\) is the realized \\(D\\) between loci \\(A\\) and \\(B\\). For example, given \\(P_A=0.1\\) and \\(P_B=0.5\\), \\(min(P_AP_b, P_BP_a)=0.05\\). Then, if the realized \\(\\tilde{D}=0.02\\) between \\(A\\) and \\(B\\), \\(D^{\\prime}=0.4\\). More discussion about LD metrics please reference to Devlin &amp; Risch (\\(Genomics\\), 1995, 29:311-22). 7.3 LD and recombination fraction It is sometimes useful to simulate \\(F_2\\) population given known LD between any pair of markers. Below is the procedure for doing that. For a pair of loci, the correlation is the function of their recombination function \\[\\rho_{1,2}=1-2c_{1,2}\\] In addition, the correlation is also the function of LD \\[\\rho_{1,2}=\\frac{D_{1,2}}{\\sqrt{P_AP_aP_BP_b}}\\] So given the allele frequency of 0.5 and the markers are evenly distributed, \\(c_{1,2}=0.5-2D_{1,2}\\), and their corresponding genetic distance is \\(d_{1,2}=-\\frac{1}{2}ln(1-2c_{1,2})=-\\frac{1}{2}ln(4D_{1,2})\\) given Haldane map function. Under the Haldane map function, the recombination between locus 1 and 3 is \\(c_{1,3}=\\frac{1}{2}[1-e^{-2(d_{1,2}+d_{2,3})}]=\\frac{1}{2}[1-e^{2ln(4D_{1,2})}]\\) Then the correlation between the locus 1 and 3 is \\(\\rho_{1,3}=1-2c_{1,3}=e^{2ln(4D_{1,2})}\\) And, their corresponding LD is \\(D_{1,3}=\\sqrt{P_AP_aP_BP_b}e^{ln(4D_{1,2})}=0.25e^{2ln(4D_{1,2})}\\) In general, \\(D_{l_1l_2}=0.25e^{|l_1-l_2|ln(4D_{1,2})}\\). A = seq(0.01, 0.5, 0.01) B = seq(0.01, 0.5, 0.01) C = 0.8 M = matrix(0, length(A), length(B)) plot(x = NULL, y = NULL, xlim = c(0, 0.5), ylim = c(0, 0.5), xlab = &quot;MAF Locus 1&quot;, ylab = &quot;MAF Locus 2&quot;, main = &quot;the relationship between D&quot;) for (i in 1:length(A)) { for (j in 1:length(B)) { M[i, j] = C * sqrt(A[i] * (1 - A[i]) * B[j] * (1 - B[j])) points(A[i], B[j], col = rgb(M[i, j]/0.2, green = 0, blue = 0), pch = 16, cex = 0.3) } } 7.4 \\(F_{st}\\) There are three commonly used \\(F_{st}\\) (see method section in Bhatia, Genome Res, 2013, 23:1514-21), 1 Weir &amp; Cockerham’s \\(F_{st}^{WC}\\) Citation: Weir &amp; Cockerham, \\(Evolution\\), 1984, 38:1358-1370 The estimator is \\(\\hat{F}_{st}^{WC}=1-\\frac{2\\frac{n_1n_2}{n_1+n_2}\\frac{1}{n_1+n_2-2}[n_1\\tilde{p}_1(1-\\tilde{p}_1)+n_2\\tilde{p}_2(1-\\tilde{p}_2))]}{\\frac{n_1n_2}{n_1+n_2}(\\tilde{p_1}-\\tilde{p_2})^2+(2\\frac{n_1n_2}{n_1+n_2}-1)\\frac{1}{n_1+n_2-2}[n_1\\tilde{p}_1(1-\\tilde{p}_1)+n_2\\tilde{p}_2(1-\\tilde{p}_2)]}\\) 2 Nei’s \\(F_{st}^{Nei}\\) Citation: Nei M. 1986. Definition and estimation of fixation indices. \\(Evolution\\), 40: 643???645. The estimator is \\(\\hat{F}_{st}^{Nei}=\\frac{(\\tilde{p}_1-\\tilde{p}_2)^2}{2\\tilde{p}(1-\\tilde{p})}\\) in which \\(\\tilde{p}=\\frac{\\tilde{p}_1+\\tilde{p}_2}{2}\\). 3 Hudson’s \\(F_{st}^{Hudson}\\) Citation: Hudson et al, Estimating of levels of gene flow from DNA sequence data. \\(Genetics\\), 132:207-11 \\(F_{st}^{Hudson} = 1-\\frac{H_w}{H_b}\\) and its estimator is \\(\\hat{F}_{st}^{Hudson} = \\frac{(\\tilde{p}_1-\\tilde{p}_2)^2-\\frac{\\tilde{p}_1(1-\\tilde{p}_1)}{n_1-1}-\\frac{\\tilde{p}_2(1-\\tilde{p}_2)}{n_2-1}}{\\tilde{p}_1(1-\\tilde{p}_1)+\\tilde{p}_2(1-\\tilde{p}_2)}\\) "],
["references.html", "References", " References "],
["notes-1.html", "Chapter 8 Notes 8.1 APY 8.2 LD score regression 8.3 Matrix inversion wiki", " Chapter 8 Notes 8.1 APY APY is application of blockwise matrix inversion, or wiki link \\[ \\begin{bmatrix} P_{11} &amp; P_{12}\\\\ P_{21} &amp; P_{22} \\end{bmatrix} ^{-1} = \\begin{bmatrix} P_{11}^-1+P_{11}^{-1}P_{12}F^{-1}P_{21}P_{11}^{-1}&amp;P_{11}^{-1}P_{12}F^{-1}\\\\ F^{-1}P_{21}P_{11}^{-1}&amp;F^{-1} \\end{bmatrix}\\] in which \\(F=P_{22}-P_{21}P_{11}^{-1}P_{12}\\) is assumed nonsingular. A much original idea of APY resembles Quaas and Pollak (J Anim Sci, 1980, 51:1277-87), an example of which can be found in Lynch&amp;Walsh page 759-61. \\(G^{-1}= \\left[ \\begin{matrix} I&amp; -P_{cn}\\\\ 0&amp;I \\end{matrix} \\right] \\left[ \\begin{matrix} G_{cc}^{-1}&amp; 0\\\\ 0&amp;M_{nn}^{-1} \\end{matrix} \\right] \\left[ \\begin{matrix} I&amp; 0\\\\ -P_{nc}&amp;I \\end{matrix} \\right] = \\left[ \\begin{matrix}G_{cc}^{-1}&amp;-P_{cn}M_{nn}^{-1}\\\\ 0&amp;M^{-1}_{nn} \\end{matrix} \\right] \\left[ \\begin{matrix}I&amp;0\\\\ -P_{nc}&amp;I \\end{matrix} \\right]\\) Citation: Misztal, I, 2016, Inexpensive computation of the inverse of the genomic relationship matrix in populations with small effective population size, \\(Genetics\\), 202:401-409. in which \\(G\\) is the numerical relationship matrix, and \\(P_{cn}=G_{cc}^{-1}G_{cn}\\) and \\(P_{nc}=G_{nc}G_{cc}^{-1}\\). \\(G_{cc}\\), \\(G_{cn}\\), and \\(G_{nc}\\) (\\(G_{cn}=G_{nc}\\)) are, “core” to “core”, “core” to “non-core”, and “non-core” to “core” relationship matrix. \\(M_{nn}=diag(G_{nn})-diag(P_{cn}^TG_{cn})\\) 8.1.1 A numerical example This example is taken from appendix in Misztal’s paper. \\(G=\\left[ \\begin{matrix} 0.81 &amp; 0 &amp; 0 &amp; 0.80 &amp; -0.80\\\\ &amp;0.81 &amp; 0 &amp; 0.80 &amp;-0.80\\\\ &amp; &amp; 0.01&amp; 0 &amp; 0\\\\ &amp; &amp; &amp; 1.61 &amp; -1.60\\\\ symm. &amp; &amp; &amp; &amp; 1.61 \\end{matrix} \\right]\\) and \\(G_{cc}^{-1}=\\left[ \\begin{matrix} 1.235 &amp; 0\\\\ 0 &amp; 1.235 \\end{matrix} \\right]\\) \\(G_{cn}=G_{nc}^T=\\left [ \\begin{matrix} 0 &amp; 0.80 &amp; -0.80\\\\ 0 &amp; 0.80 &amp; -0.80 \\end{matrix} \\right]\\) and \\(P_{cn}=G_{cc}^{-1}G_{cn}=\\left [ \\begin{matrix} 0.00 &amp; 0.988 &amp; -0.988\\\\ 0.00 &amp; 0.988 &amp; -0.988 \\end{matrix} \\right ]\\) \\(M_{nn}=diag(G_{nn})-diag(P_{cn}^TG_{cn}) = \\left [ \\begin{matrix}0.01 &amp; &amp;\\\\ &amp; 1.61 &amp; \\\\ &amp; &amp; 1.61 \\end{matrix} \\right] - \\left [ \\begin{matrix}0.00 &amp; &amp;\\\\ &amp; 1.58 &amp; \\\\ &amp; &amp; 1.58 \\end{matrix} \\right]=\\left [ \\begin{matrix}0.01 &amp; &amp;\\\\ &amp; 0.03 &amp; \\\\ &amp; &amp; 0.03 \\end{matrix} \\right]\\) and \\(G^{-1}=\\left[ \\begin{matrix} 66.8 &amp; 65.5 &amp; 0 &amp; -33.1 &amp; 33.1\\\\ &amp;66.804 &amp; 0 &amp; -33.1 &amp; 33.1\\\\ &amp; &amp; 100&amp; 0 &amp; 0\\\\ &amp; &amp; &amp; 33.6 &amp; 0\\\\ symm. &amp; &amp; &amp; &amp; 33.6 \\end{matrix} \\right]\\) This is APY inversed \\(G\\), and for a comparison, a regular inverse of \\(G\\) is quit different \\(G^{-1}_{reg}=\\left[ \\begin{matrix} 40.6 &amp; 39.4 &amp; 0 &amp; -19.9 &amp; 19.9\\\\ &amp;40.6 &amp; 0 &amp; -19.9 &amp; 19.9\\\\ &amp; &amp; 100&amp; 0 &amp; 0\\\\ &amp; &amp; &amp; 60.0 &amp; 39.9\\\\ symm. &amp; &amp; &amp; &amp; 60.0 \\end{matrix} \\right]\\) GRM_C=matrix(c(0.81, 0, 0, 0.81), 2, 2, byrow = T) GRM_N=matrix(0, 3, 3) diag(GRM_N)=c(.01, 1.61, 1.61) GRM_N[2,3]=GRM_N[3,2]=-1.6 GRM_CN=matrix(0, 2, 3) GRM_CN[,2]=0.8 GRM_CN[,3]=-0.8 GRM=rbind(cbind(GRM_C, GRM_CN), cbind(t(GRM_CN), GRM_N)) print(GRM) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.81 0.00 0.00 0.80 -0.80 ## [2,] 0.00 0.81 0.00 0.80 -0.80 ## [3,] 0.00 0.00 0.01 0.00 0.00 ## [4,] 0.80 0.80 0.00 1.61 -1.60 ## [5,] -0.80 -0.80 0.00 -1.60 1.61 print(solve(GRM)) #original ## [,1] [,2] [,3] [,4] [,5] ## [1,] 40.64222 39.40765 0 -19.95012 19.95012 ## [2,] 39.40765 40.64222 0 -19.95012 19.95012 ## [3,] 0.00000 0.00000 100 0.00000 0.00000 ## [4,] -19.95012 -19.95012 0 60.09975 39.90025 ## [5,] 19.95012 19.95012 0 39.90025 60.09975 IC=solve(GRM_C) Pcn=IC %*% GRM_CN Pnc=t(GRM_CN) %*% IC Pcn_Gcn=t(Pcn) %*% GRM_CN Mnn=diag(diag(GRM_N)-diag(Pcn_Gcn), nrow=nrow(GRM_N), ncol=nrow(GRM_N)) IMnn=solve(Mnn) v1_1=rbind(IC, matrix(0, nrow=nrow(GRM_N), ncol=ncol(GRM_C))) v1_2=rbind(-1*Pcn%*%IMnn, IMnn) V1=cbind(v1_1, v1_2) v2_1=rbind(diag(1, nrow=nrow(GRM_C), ncol=nrow(GRM_C)), -1*Pnc) v2_2=rbind(matrix(0, nrow=nrow(GRM_C), ncol=nrow(GRM_N)), diag(1, nrow(GRM_N), ncol(GRM_N))) V2=cbind(v2_1, v2_2) IV=V1 %*% V2 print(IV) #apy ## [,1] [,2] [,3] [,4] [,5] ## [1,] 66.80498 65.57041 0 -33.19502 33.19502 ## [2,] 65.57041 66.80498 0 -33.19502 33.19502 ## [3,] 0.00000 0.00000 100 0.00000 0.00000 ## [4,] -33.19502 -33.19502 0 33.60996 0.00000 ## [5,] 33.19502 33.19502 0 0.00000 33.60996 IVI=solve(IV) print(IVI) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 8.100000e-01 9.572467e-16 0.00 0.800000 -0.800000 ## [2,] 1.089201e-15 8.100000e-01 0.00 0.800000 -0.800000 ## [3,] 0.000000e+00 0.000000e+00 0.01 0.000000 0.000000 ## [4,] 8.000000e-01 8.000000e-01 0.00 1.610000 -1.580247 ## [5,] -8.000000e-01 -8.000000e-01 0.00 -1.580247 1.610000 8.2 LD score regression source(&quot;~/R/MyLib/shotgun.R&quot;) REP = 50 N = 500 M = 1000 h2 = 0.5 h2E = array(0, dim = c(2, REP)) for (rep in 1:REP) { fq = runif(M, 0.05, 0.95) Dl = array(0, dim = M) Dl = runif(M, 0.8, 0.9) Dl[seq(10, M, 10)] = 0 G = GenerateGenoDprime(fq, Dl[1:(length(Dl) - 1)], N) b = rnorm(M, 0, sqrt(h2/M)) bv = G %*% b Y = bv + rnorm(N, 0, 1) chi = array(0, dim = M) for (i in 1:M) { sm = summary(lm(Y ~ G[, i])) chi[i] = sm$coefficients[2, 3]^2 } plot(chi, b^2 * fq * (1 - fq), pch = 16) lds = array(0, dim = M) for (i in 1:(M/10)) { cg = cor(G[, ((i - 1) * 10 + 1):(i * 10)]) for (j in 1:10) { lds[(i - 1) * 10 + j] = sum(cg[, j]^2) } } ldSc = lm(chi ~ lds) h2E[1, rep] = ldSc$coefficients[2] * M/N h2E[2, rep] = var(bv)/var(Y) } plot(h2E[1, ], h2E[2, ]) 8.3 Matrix inversion wiki 8.3.1 2 X 2 The cofactor equation listed above yields the following result for 2 ?? 2 matrices. Inversion of these matrices can be done as follows \\[\\text{A}^{-1}=\\left[ \\begin{matrix} a &amp; b\\\\ c &amp; d \\end{matrix} \\right] ^{-1} =\\frac{1}{det (\\text{A})}\\left[ \\begin{matrix} A &amp; B\\\\ C &amp; D \\end{matrix} \\right] =\\frac{1}{ad-bc}\\left[ \\begin{matrix} d &amp; -b\\\\ -c &amp; a \\end{matrix} \\right] \\] Where the scalar \\(A\\) is not to be confused with the matrix A. 8.3.2 3 X 3 \\[\\text{A}^{-1}=\\left[ \\begin{matrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{matrix} \\right] ^{-1} =\\frac{1}{det (\\text{A})}\\left[ \\begin{matrix} A &amp; B &amp; C\\\\ D &amp; E &amp; F\\\\ G &amp; H &amp; I \\end{matrix} \\right] ^{T} =\\frac{1}{det (\\text{A})}\\left[ \\begin{matrix} A &amp; D &amp; G\\\\ B &amp; E &amp; H\\\\ C &amp; F &amp; I \\end{matrix} \\right] \\] in which \\[\\begin{matrix} A=(ei-fh), &amp; D=-(bi-ch), &amp; G = -(bf-ce), \\\\ B=-(di-fg), &amp; E=(ai-cg), &amp; H = -(af-cd), \\\\ C=(dh-eg), &amp; F=-(ah-bg), &amp; I = (ae-bd). \\end{matrix} \\] "],
["IBD-fam.html", "Chapter 9 IBD notes 9.1 HE association 9.2 IBD table", " Chapter 9 IBD notes 9.1 HE association The classic \\(h^2=\\beta HLH\\beta\\) In population-based design, the relatedness is measured in IBS \\(h^2=\\beta HLH\\beta\\) \\(h^2_d=d H_dL_dH_dd\\) \\(h^2_{SNP.A}=m\\beta H\\{\\frac{ \\sum_{k=1}^{m} v_k^Tv_k }{1^TP1}\\} H \\beta\\) \\(h^2_{SNP.D}=md H_d\\{\\frac{ \\sum_{k=1}^{m} v_{k,d}^T v_{k,d} }{1^TP_d1}\\} H_d d\\) \\(\\tilde{h}^2_{SNP.A}=m\\beta H\\{\\frac{ \\sum_{k=1}^{m}w_k v_{k}^Tv_k }{w^TPw}\\} H \\beta\\) \\(\\tilde{h}^2_{SNP.D}=md H_d\\{\\frac{ \\sum_{k=1}^{m} w_{k,d}v_{k.d}^Tv_{k.d} }{w_{d}^TP_dw_{d}}\\} H_d d\\) \\(h^2_{SNP.A_2}=m\\beta_1 H_1\\{\\frac{ \\sum_{k=1}^{m} v_{k_1}^Tv_{k_2} }{1^TP1}\\} H_2 \\beta_2\\) \\(h^2_{SNP.D_2}=md_1 H_{d_1}\\{\\frac{ \\sum_{k=1}^{m} v_{k,d_1}^Tv_{k,d_2} }{1^TP_d1}\\} H_{d_2} d_2\\) \\(\\tilde{h}^2_{SNP.A_2}=m\\beta_1 H_1\\{\\frac{ \\sum_{k=1}^{m}w_k v_{k,1}^Tv_{k,2} }{w^TPw}\\} H_2 \\beta_2\\) \\(\\tilde{h}^2_{SNP.D_2}=md_1 H_{d_1}\\{\\frac{ \\sum_{k=1}^{m} w_{k,d}v_{k,d_1}^Tv_{k,d_2} }{w_{d}^TP_dw_{d}}\\} H_{d_2} d_2\\) In sibpair design, using IBD \\[h^2_{fam}=m\\beta H\\{\\frac{\\sum_{k=1}^mz^T_kz_k}{\\sum_{k_1=1}^m\\sum_{k_2=1}^m (1-2c_{k_1k_2})^2}\\}H\\beta\\] \\(z_k=[(1-2c_{k,1}), (1-2c_{k,2}), (1-2c_{k,3}),...,(1-2c_{k,l})]\\) 9.2 IBD table IBD \\((freq)\\) Mating type (\\(freq\\)) Sib pair (\\(freq\\)) \\(\\Omega\\) IBD=1 IBD=\\(\\frac{1}{2}\\) IBD=0 \\(E(IBD)\\) \\(AA , AA\\) (\\(p^4\\)) \\(\\color{red}{\\{AA, AA\\}}\\) (1) \\(\\color{red}{\\frac{4q^2}{2pq}}\\) \\(\\color{red}{\\frac{1}{4}}\\) \\(\\color{red}{\\frac{1}{2}}\\) \\(\\color{red}{\\frac{1}{4}}\\) \\(\\color{red}{\\frac{1}{2}}\\) \\(AA , Aa\\) (\\(4p^3q\\)) \\(\\color{red}{\\{AA, AA\\}}\\) (\\(\\frac{1}{4}\\)) \\(\\color{red}{\\frac{4q^2}{2pq}}\\) \\(\\color{red}{\\frac{1}{2}}\\) \\(\\color{red}{\\frac{1}{2}}\\) \\(\\color{red}{\\frac{3}{4}}\\) \\(\\{AA, Aa\\}\\) (\\(\\frac{1}{2}\\)) \\(\\frac{2q(q-p)}{2pq}\\) \\(\\frac{1}{2}\\) \\(\\frac{1}{2}\\) \\(\\frac{1}{4}\\) \\(\\color{green}{\\{Aa, Aa\\}}\\) (\\(\\frac{1}{4}\\)) \\(\\color{green}{\\frac{(q-p)^2}{2pq}}\\) \\(\\color{green}{\\frac{1}{2}}\\) \\(\\color{green}{\\frac{1}{2}}\\) \\(\\color{green}{\\frac{3}{4}}\\) \\(AA , aa\\) (\\(2p^2q^2\\)) \\(\\color{green}{\\{Aa, Aa\\}}\\) (1) \\(\\color{green}{\\frac{(q-p)^2}{2pq}}\\) \\(\\color{green}{\\frac{1}{4}}\\) \\(\\color{green}{\\frac{1}{2}}\\) \\(\\color{green}{\\frac{1}{4}}\\) \\(\\color{green}{\\frac{1}{2}}\\) \\(Aa , Aa\\) (\\(4p^2q^2\\)) \\(\\color{red}{\\{AA, AA \\}}\\) (\\(\\frac{1}{16}\\)) \\(\\color{red}{\\frac{4q^2}{2pq}}\\) \\(\\color{red}{1}\\) \\(\\color{red}{1}\\) \\(\\{AA, Aa \\}\\) (\\(\\frac{1}{4}\\)) \\(\\frac{2q(q-p)}{2pq}\\) \\(1\\) \\(\\frac{1}{2}\\) \\(\\color{grey}{\\{AA, aa\\}}\\) (\\(\\frac{1}{8}\\)) \\(\\color{grey}{\\frac{-4pq}{2pq}}\\) \\(\\color{grey}{1}\\) \\(\\color{grey}{0}\\) \\(\\color{green}{\\{Aa, Aa\\}}\\) (\\(\\frac{1}{4}\\)) \\(\\color{green}{\\frac{(q-p)^2}{2pq}}\\) \\(\\color{green}{\\frac{1}{2}}\\) \\(\\color{green}{\\frac{1}{2}}\\) \\(\\color{green}{\\frac{1}{2}}\\) \\(\\color{gold}{\\{Aa, aa \\}}\\) (\\(\\frac{1}{4}\\)) \\(\\color{gold}{\\frac{-2p(q-p)}{2pq}}\\) \\(\\color{gold}{1}\\) \\(\\color{gold}{\\frac{1}{2}}\\) \\(\\color{blue}{\\{aa, aa\\}}\\) (\\(\\frac{1}{16}\\)) \\(\\color{blue}{\\frac{4p^2}{2pq}}\\) \\(\\color{blue}{1}\\) \\(\\color{blue}{1}\\) \\(Aa, aa\\) (\\(4pq^3\\)) \\(\\color{green}{\\{Aa, Aa\\}}\\) (\\(\\frac{1}{4}\\)) \\(\\color{green}{\\frac{(q-p)^2}{2pq}}\\) \\(\\color{green}{\\frac{1}{2}}\\) \\(\\color{green}{\\frac{1}{2}}\\) \\(\\color{green}{\\frac{3}{4}}\\) \\(\\color{gold}{\\{Aa, aa\\}}\\) (\\(\\frac{1}{2}\\)) \\(\\color{gold}{\\frac{-2p(q-p)}{2pq}}\\) \\(\\color{gold}{\\frac{1}{2}}\\) \\(\\color{gold}{\\frac{1}{2}}\\) \\(\\color{gold}{\\frac{1}{4}}\\) \\(\\color{blue}{\\{aa, aa\\}}\\) (\\(\\frac{1}{4}\\)) \\(\\color{blue}{\\frac{4p^2}{2pq}}\\) \\(\\color{blue}{\\frac{1}{2}}\\) \\(\\color{blue}{\\frac{1}{2}}\\) \\(\\color{blue}{\\frac{3}{4}}\\) \\(aa, aa\\) (\\(q^4\\)) \\(\\color{blue}{\\{aa, aa\\}}\\) (1) \\(\\color{blue}{\\frac{4p^2}{2pq}}\\) \\(\\color{blue}{\\frac{1}{4}}\\) \\(\\color{blue}{\\frac{1}{2}}\\) \\(\\color{blue}{\\frac{1}{4}}\\) \\(\\color{blue}{\\frac{1}{2}}\\) \\(E(IBD)=\\begin{matrix} \\frac{1}{2}p^4&amp;\\\\ +(\\frac{3}{4}\\frac{1}{4}+\\frac{1}{4}\\frac{1}{2}+\\frac{3}{4}\\frac{1}{4})4p^3q&amp;\\\\ +\\frac{1}{2}2p^2q^2&amp;\\\\ +(1\\cdot \\frac{1}{16}+\\frac{1}{2}\\frac{1}{4}+0\\cdot \\frac{1}{8}+\\frac{1}{2}\\frac{1}{4}+\\frac{1}{2}\\frac{1}{4}+1\\cdot \\frac{1}{16})4p^2q^2&amp;\\\\ +(\\frac{3}{4}\\frac{1}{4}+\\frac{1}{4}\\frac{1}{2}+\\frac{3}{4}\\frac{1}{4})4pq^3&amp;\\\\ +\\frac{1}{2}q^4 \\end{matrix} =\\frac{1}{2}\\) The variance of IBD is \\(var(IBD)=\\frac{p^4}{8} + \\frac{p^3q}{2} + \\frac{p^2q^2}{4} + \\frac{3p^2q^2}{2} + \\frac{pq^3}{2} + \\frac{q^4}{8}=\\frac{1}{8}+p^2q^2\\). However, if the IBD is known for sure, then \\(var(IBD)=\\frac{1}{8}\\). The expectation of \\(\\Omega\\) is \\(E(IBS) = 2pq[(p+\\frac{q}{2})^2+(\\frac{p}{2}+q)^2]-pq(p-q)^2-p^2q^2+\\frac{(q-p)^2}{2}(1+pq) =\\frac{1}{2}\\) The variance of \\(\\Omega\\) is \\(var(IBS) = E(\\Omega^2)-E(\\Omega)^2= \\frac{p^6+7p^5q+19p^4q^2+26p^3q^3+19p^2q^4+7pq^5+q^6}{4pq} - \\frac{1}{4}\\) Mating type Frequency \\(IBD \\times IBS\\) \\(AA \\times AA\\) \\(p^4\\) \\(\\frac{1}{2}\\frac{4q^2}{2pq}p^4\\) \\(aa \\times aa\\) \\(q^4\\) \\(\\frac{1}{2}\\frac{4p^2}{2pq}q^4\\) \\(AA \\times aa\\) \\(2p^2q^2\\) \\(\\frac{1}{2}\\frac{(q-p)^2}{2pq}p^4\\) \\(AA \\times Aa\\) \\(4p^3q\\) \\(\\{\\frac{3}{16}\\frac{4q^2}{2pq} + \\frac{1}{8}\\frac{2q(p-q)}{2pq} + \\frac{3}{16}\\frac{(q-p)^2}{2pq}\\}4p^3q\\) \\(Aa \\times aa\\) \\(4pq^3\\) \\(\\{\\frac{3}{16}\\frac{4q^2}{2pq} + \\frac{1}{8}\\frac{2p(p-q)}{2pq} + \\frac{3}{16}\\frac{(q-p)^2}{2pq}\\}4pq^3\\) \\(Aa \\times Aa\\) \\(4p^2q^2\\) \\(\\{\\frac{1}{16}\\frac{4q^2}{2pq} + \\frac{1}{16}\\frac{4p^2}{2pq}+ \\frac{1}{8}\\frac{2q(p-q)}{2pq}-\\frac{1}{8}\\frac{2p(p-q)}{2pq} + \\frac{1}{8}\\frac{(q-p)^2}{2pq}\\}4p^2q^2\\) The \\(cov(IBD,IBS)=E(IBD\\times IBS) - E(IBD)E(IBS)=\\frac{3}{8}-\\frac{1}{2}\\frac{1}{2}=\\frac{1}{8}\\), and \\(cor(IBD, IBS)=\\frac{cov(IBS, IBD)}{\\sqrt{var(IBD)var(IBS)}}=\\frac{1}{\\sqrt{8var(IBS)}}\\). It indicates that for sibpairs IBD and IBS are correlated for a locus. "],
["app-2.html", "Chapter 10 Appendix quick pca 10.1 Quck PCA method for snp matrix", " Chapter 10 Appendix quick pca 10.1 Quck PCA method for snp matrix See PLoS ONE, 2014, 9:e93766 library(MASS) N=1000 #sample size M=2000 #SNP X=matrix(0, N, M) #SNP matrix #simulating snp for(i in 1:M) { p1=runif(1, 0.1, 0.9) p2=1-p1 X[1:(N/2),i]=rbinom(N/2, 2, p1) X[(N/2+1):N,i]=rbinom(N/2, 2, p2) } #conventional PCA sX=scale(X) G=sX%*%t(sX) Geg=eigen(G) #plot(Geg$vectors[,1], Geg$vectors[,2]) #quick pca dm=30 sg=matrix(0,dm,dm) diag(sg)=1 R=mvrnorm(M, rep(0, dm), Sigma=sg) xt=sX%*%R ss=apply(xt^2, 2, sum) Y=matrix(0, nrow(xt), ncol(xt)) for(i in 1:length(ss)) { Y[,i]=xt[,i]/ss[i] } XtX=sX%*%t(sX) maxiter=10 for(it in 1:maxiter) { xxt=XtX%*%Y ss1=apply(xxt^2, 2, sum) for(i in 1:length(ss1)) { Y[,i]=xxt[,i]/ss1[i] } } QR=qr.default(Y) B=t(QR$qr)%*%sX S=B%*%t(B) eg=eigen(S) U=QR$qr %*% eg$vectors D=sqrt(eg$values/(N-1)) P=U*D par(mfrow = c(1,2)) plot(main=&quot;PCA&quot;, xlab=&quot;eVec 1&quot;, ylab=&quot;eVec 2&quot;, Geg$vectors[,1], Geg$vectors[,2]) plot(main=&quot;Quick PCA&quot;, xlab=&quot;eVec 1&quot;, ylab=&quot;eVec 2&quot;, U[,1], U[,2]) cor(Geg$vectors[,1], U[,1]) ## [1] 0.9929008 The procedure in Galinsky (AJHG) library(Rcpp) sourceCpp(&quot;~/git/Notes/R/RLib/Shotgun.cpp&quot;) M=10000 N=500 L=20 I=5 frq=runif(M, 0.1, 0.3) Dp=sample(c(runif(M/2, 0, 0), runif(M/2, 0, 0)), M) Dp=Dp[-1] fst=0.02 frq1=rbeta(M, frq*(1-fst)/fst, (1-frq)*(1-fst)/fst) frq2=rbeta(M, frq*(1-fst)/fst, (1-frq)*(1-fst)/fst) G1=GenerateGenoDprimeRcpp(frq1, Dp, N) G2=GenerateGenoDprimeRcpp(frq2, Dp, N) G=rbind(G1, G2) s=apply(G, 2, scale) ss=s%*%t(s)/M sE=eigen(ss) G0=matrix(rnorm(nrow(s)*L), nrow(s), L) HH=matrix(0, M, (I+1)*L) for(i in 0:(I-1)) { H=t(s) %*% G0 G0=s%*%H/M HH[,(i*L+1):((i+1)*L)]=H } svd_h=svd(HH) Ty=t(svd_h$u)%*%t(s) svd_t=svd(Ty) layout(matrix(1:2, 1, 2)) plot(sE$vectors[,1], sE$vectors[,2]) plot(svd_t$v[,1], svd_t$v[,2], col=&quot;red&quot;) "]
]
